{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdab3e0-3174-4f16-9e6e-ba7b304f6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Dropout, Activation, Input, Conv2D, BatchNormalization, ReLU,\n",
    "    MaxPooling2D, UpSampling2D, Concatenate, Multiply, Add\n",
    ")\n",
    "from tensorflow.keras.models import ModelQ\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.losses import Loss\n",
    "import supervisely as sly\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import adapted_rand_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy import ndimage\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# pielāgošana\n",
    "class Config:\n",
    "    IMAGE_DIR = \"/mnt/c/dataset_folder/ds/img/\"\n",
    "    MASK_DIR = \"/mnt/c/dataset_folder/ds/ann/\"\n",
    "    MODEL_DIR = \"/mnt/c/saved_models/\"\n",
    "    IMG_SIZE = (256, 256)\n",
    "    BATCH = 3\n",
    "    AUTO_THRESH = False\n",
    "\n",
    "os.makedirs(Config.MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb42031-be61-4371-bfa8-6280473addb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datu ielāde\n",
    "def load_images_and_masks(img_dir, mask_dir, meta, target_size):\n",
    "    images, masks = [], []\n",
    "    for img_name in tqdm(os.listdir(img_dir), desc=\"Loading images\"):\n",
    "        if not img_name.endswith(\".png\"): continue\n",
    "        \n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        orig_img = sly.image.read(img_path)\n",
    "        h, w = orig_img.shape[:2]\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        mask_path = os.path.join(mask_dir, img_name + \".json\")\n",
    "        if os.path.exists(mask_path):\n",
    "            with open(mask_path) as f:\n",
    "                ann = sly.Annotation.from_json(json.load(f), meta)\n",
    "            for label in ann.labels:\n",
    "                try:\n",
    "                    label.draw(mask, color=255)\n",
    "                except Exception as e:\n",
    "                    print(f\"Couldn't draw label on {img_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        image = cv2.resize(orig_img, target_size) / 255.0\n",
    "        mask = cv2.resize(mask, target_size, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        masks.append(mask.astype(np.float32)[..., None] / 255.0)\n",
    "        images.append(image)\n",
    "    \n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "with open(os.path.join(Config.MASK_DIR, \"meta.json\")) as f:\n",
    "    meta = sly.ProjectMeta.from_json(json.load(f))\n",
    "\n",
    "images, masks = load_images_and_masks(Config.IMAGE_DIR, Config.MASK_DIR, meta, Config.IMG_SIZE)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7091e13-15e2-4065-8a52-fa722299f175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modeļa arhitektūra\n",
    "def unet_model(input_size=(256, 256, 3)):\n",
    "    inputs = layers.Input(input_size)\n",
    "    \n",
    "    e1 = Conv2D(32, 3, padding='same')(inputs)\n",
    "    e1 = BatchNormalization()(e1)\n",
    "    e1 = Activation('relu')(e1)\n",
    "    e1 = Conv2D(32, 3, padding='same')(e1)\n",
    "    e1 = BatchNormalization()(e1)\n",
    "    e1 = Activation('relu')(e1)\n",
    "    p1 = MaxPooling2D()(e1)\n",
    "    p1 = Dropout(0.1)(p1)\n",
    "    \n",
    "    e2 = Conv2D(64, 3, padding='same')(p1)\n",
    "    e2 = BatchNormalization()(e2)\n",
    "    e2 = Activation('relu')(e2)\n",
    "    e2 = Conv2D(64, 3, padding='same')(e2)\n",
    "    e2 = BatchNormalization()(e2)\n",
    "    e2 = Activation('relu')(e2)\n",
    "    p2 = MaxPooling2D()(e2)\n",
    "    p2 = Dropout(0.2)(p2)\n",
    "    \n",
    "    e3 = Conv2D(128, 3, padding='same')(p2)\n",
    "    e3 = BatchNormalization()(e3)\n",
    "    e3 = Activation('relu')(e3)\n",
    "    e3 = Conv2D(128, 3, padding='same')(e3)\n",
    "    e3 = BatchNormalization()(e3)\n",
    "    e3 = Activation('relu')(e3)\n",
    "    p3 = MaxPooling2D()(e3)\n",
    "    p3 = Dropout(0.3)(p3)\n",
    "    \n",
    "    m = Conv2D(256, 3, dilation_rate=2, padding='same')(p3)\n",
    "    m = BatchNormalization()(m)\n",
    "    m = Activation('relu')(m)\n",
    "    m = Conv2D(256, 3, dilation_rate=2, padding='same')(m)\n",
    "    m = BatchNormalization()(m)\n",
    "    m = Activation('relu')(m)\n",
    "    m = Dropout(0.4)(m)\n",
    "    \n",
    "    d1 = UpSampling2D()(m)\n",
    "    d1 = Conv2D(128, 2, padding='same')(d1)\n",
    "    d1 = BatchNormalization()(d1)\n",
    "    d1 = Activation('relu')(d1)\n",
    "    d1 = Concatenate()([d1, e3])\n",
    "    d1 = Conv2D(128, 3, padding='same')(d1)\n",
    "    d1 = BatchNormalization()(d1)\n",
    "    d1 = Activation('relu')(d1)\n",
    "    d1 = Conv2D(128, 3, padding='same')(d1)\n",
    "    d1 = BatchNormalization()(d1)\n",
    "    d1 = Activation('relu')(d1)\n",
    "    \n",
    "    d2 = UpSampling2D()(d1)\n",
    "    d2 = Conv2D(64, 2, padding='same')(d2)\n",
    "    d2 = BatchNormalization()(d2)\n",
    "    d2 = Activation('relu')(d2)\n",
    "    d2 = Concatenate()([d2, e2])\n",
    "    d2 = Conv2D(64, 3, padding='same')(d2)\n",
    "    d2 = BatchNormalization()(d2)\n",
    "    d2 = Activation('relu')(d2)\n",
    "    d2 = Conv2D(64, 3, padding='same')(d2)\n",
    "    d2 = BatchNormalization()(d2)\n",
    "    d2 = Activation('relu')(d2)\n",
    "    \n",
    "    d3 = UpSampling2D()(d2)\n",
    "    d3 = Conv2D(32, 2, padding='same')(d3)\n",
    "    d3 = BatchNormalization()(d3)\n",
    "    d3 = Activation('relu')(d3)\n",
    "    d3 = Concatenate()([d3, e1])\n",
    "    d3 = Conv2D(32, 3, padding='same')(d3)\n",
    "    d3 = BatchNormalization()(d3)\n",
    "    d3 = Activation('relu')(d3)\n",
    "    d3 = Conv2D(32, 3, padding='same')(d3)\n",
    "    d3 = BatchNormalization()(d3)\n",
    "    d3 = Activation('relu')(d3)\n",
    "    \n",
    "    attention = Conv2D(1, 1, activation='sigmoid')(d3)\n",
    "    d3 = Multiply()([d3, attention])\n",
    "    \n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(d3)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "class FocusedSegmentationLoss(Loss):\n",
    "    def __init__(self, alpha=0.8, beta=0.2, gamma=1.5, name='focused_loss'):\n",
    "        super().__init__(name=name)\n",
    "        self.alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "        self.beta = tf.constant(beta, dtype=tf.float32)\n",
    "        self.gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "        self.smooth = tf.constant(1e-6, dtype=tf.float32)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        tp = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "        fp = tf.reduce_sum((1 - y_true) * y_pred, axis=[1, 2, 3])\n",
    "        fn = tf.reduce_sum(y_true * (1 - y_pred), axis=[1, 2, 3])\n",
    "        tversky = (tp + self.smooth) / (tp + self.alpha * fp + self.beta * fn + self.smooth)\n",
    "        focal_tversky = tf.pow(1 - tversky, self.gamma)\n",
    "        \n",
    "        if len(y_true.shape) == 4:\n",
    "            y_true_dx, y_true_dy = tf.image.image_gradients(y_true)\n",
    "            y_pred_dx, y_pred_dy = tf.image.image_gradients(y_pred)\n",
    "            boundary_loss = tf.reduce_mean(\n",
    "                tf.abs(y_true_dx - y_pred_dx) + tf.abs(y_true_dy - y_pred_dy),\n",
    "                axis=[1, 2, 3]\n",
    "            )\n",
    "        else:\n",
    "            boundary_loss = 0.0\n",
    "        \n",
    "        return focal_tversky + 0.05 * boundary_loss\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'alpha': float(self.alpha), 'beta': float(self.beta), 'gamma': float(self.gamma)}\n",
    "\n",
    "model = unet_model()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=FocusedSegmentationLoss(alpha=0.9, beta=0.1, gamma=1.5),\n",
    "    metrics=['accuracy', tf.keras.metrics.MeanIoU(2)]\n",
    ")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(Config.MODEL_DIR, 'Segment_modelis_1.keras'),\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss'\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.fit(\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(Config.BATCH),\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=25,\n",
    "        callbacks=[checkpoint]\n",
    "    )\n",
    "except tf.errors.ResourceExhaustedError:\n",
    "    print(\"Atmiņas trūkums.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad911f7-514e-4013-ae6c-d7d4d0de8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "class SegmentationEvaluator:\n",
    "    def __init__(self, model, X_test, y_test):\n",
    "        self.model = model\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.threshold = 0.1  # Default threshold\n",
    "        self.min_object_size = 100  # Minimum pixels to consider an object\n",
    "    \n",
    "    def calculate_iou(self, y_true, y_pred):\n",
    "        intersection = np.sum(y_true * y_pred)\n",
    "        union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "        return intersection / (union + 1e-7)\n",
    "    \n",
    "    def find_optimal_threshold(self, n_samples=100):\n",
    "        preds = self.model.predict(self.X_test[:n_samples], verbose=0)\n",
    "        thresholds = np.linspace(0.1, 0.9, 20)\n",
    "        best_thresh = 0.5\n",
    "        best_dice = 0\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            y_true_binary = (self.y_test[:n_samples] > thresh).astype(np.int32)\n",
    "            y_pred_binary = (preds > thresh).astype(np.int32)\n",
    "            dice = f1_score(y_true_binary.flatten(), y_pred_binary.flatten())\n",
    "            \n",
    "            if dice > best_dice:\n",
    "                best_dice = dice\n",
    "                best_thresh = thresh\n",
    "        \n",
    "        self.threshold = best_thresh\n",
    "        return best_thresh\n",
    "    \n",
    "    def _get_connected_components(self, mask):\n",
    "        labeled, num_features = ndimage.label(mask)\n",
    "        objects = []\n",
    "        for i in range(1, num_features+1):\n",
    "            obj_mask = (labeled == i)\n",
    "            if np.sum(obj_mask) >= self.min_object_size:\n",
    "                objects.append(obj_mask)\n",
    "        return objects\n",
    "    \n",
    "    def evaluate(self):\n",
    "        preds = self.model.predict(self.X_test, verbose=0)\n",
    "        \n",
    "        y_true_binary = (self.y_test > self.threshold).astype(np.int32)\n",
    "        y_pred_binary = (preds > self.threshold).astype(np.int32)\n",
    "        \n",
    "        tp = np.sum(y_true_binary * y_pred_binary)  # True positives\n",
    "        fp = np.sum((1-y_true_binary) * y_pred_binary)  # False positives\n",
    "        fn = np.sum(y_true_binary * (1-y_pred_binary))  # False negatives\n",
    "        tn = np.sum((1-y_true_binary) * (1-y_pred_binary))  # True negatives\n",
    "        \n",
    "        false_detections = 0\n",
    "        missed_detections = 0\n",
    "        good_detections = 0\n",
    "        \n",
    "        for i in range(len(self.X_test)):\n",
    "            true_objects = self._get_connected_components(y_true_binary[i])\n",
    "            pred_objects = self._get_connected_components(y_pred_binary[i])\n",
    "            \n",
    "            if len(true_objects) == 0:\n",
    "                false_detections += len(pred_objects)\n",
    "            \n",
    "            else:\n",
    "                if len(pred_objects) == 0:\n",
    "                    missed_detections += len(true_objects)\n",
    "                else:\n",
    "                    for true_obj in true_objects:\n",
    "                        max_iou = 0\n",
    "                        for pred_obj in pred_objects:\n",
    "                            iou = self.calculate_iou(true_obj, pred_obj)\n",
    "                            if iou > max_iou:\n",
    "                                max_iou = iou\n",
    "                        if max_iou > 0.5:\n",
    "                            good_detections += 1\n",
    "                        else:\n",
    "                            missed_detections += 1\n",
    "                    \n",
    "                    false_detections += max(0, len(pred_objects) - len(true_objects))\n",
    "        \n",
    "        total_objects = np.sum([len(self._get_connected_components(m)) for m in y_true_binary])\n",
    "        \n",
    "        return {\n",
    "            'pixel_accuracy': accuracy_score(y_true_binary.flatten(), y_pred_binary.flatten()),\n",
    "            'precision': precision_score(y_true_binary.flatten(), y_pred_binary.flatten()),\n",
    "            'recall': recall_score(y_true_binary.flatten(), y_pred_binary.flatten()),\n",
    "            'IoU': self.calculate_iou(y_true_binary, y_pred_binary),\n",
    "            'Dice': f1_score(y_true_binary.flatten(), y_pred_binary.flatten()),\n",
    "            \n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'true_negatives': tn,\n",
    "            'false_negatives': fn,\n",
    "            \n",
    "            'object_detection_rate': good_detections / (total_objects + 1e-7),\n",
    "            'false_discovery_rate': false_detections / (false_detections + good_detections + 1e-7),\n",
    "            'missed_object_rate': missed_detections / (total_objects + 1e-7),\n",
    "            'total_objects': total_objects,\n",
    "            'false_detections': false_detections,\n",
    "            'missed_detections': missed_detections,\n",
    "            'good_detections': good_detections,\n",
    "            \n",
    "            'threshold': self.threshold\n",
    "        }\n",
    "    \n",
    "    def visualize_samples(self, n_samples=3, show_analysis=False):\n",
    "        preds = self.model.predict(self.X_test[:n_samples], verbose=0)\n",
    "        \n",
    "        plt.figure(figsize=(18, 5*n_samples))\n",
    "        for i in range(n_samples):\n",
    "            y_true_bin = (self.y_test[i] > self.threshold).astype(np.int32)\n",
    "            y_pred_bin = (preds[i] > self.threshold).astype(np.int32)\n",
    "            \n",
    "            plt.subplot(n_samples, 4, i*4+1)\n",
    "            plt.imshow(self.X_test[i])\n",
    "            plt.title(f\"Image {i+1}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(n_samples, 4, i*4+2)\n",
    "            plt.imshow(y_true_bin.squeeze(), cmap='gray')\n",
    "            plt.title(\"True Mask\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(n_samples, 4, i*4+3)\n",
    "            plt.imshow(y_pred_bin.squeeze(), cmap='gray')\n",
    "            plt.title(f\"Predicted (Threshold={self.threshold:.2f})\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            if show_analysis:\n",
    "                plt.subplot(n_samples, 4, i*4+4)\n",
    "                error_mask = np.zeros_like(y_true_bin)\n",
    "                error_mask[(y_true_bin==1)&(y_pred_bin==0)] = 1  \n",
    "                error_mask[(y_true_bin==0)&(y_pred_bin==1)] = 2  \n",
    "                error_mask[(y_true_bin==1)&(y_pred_bin==1)] = 3  \n",
    "                \n",
    "                plt.imshow(error_mask.squeeze(), cmap='jet', vmin=0, vmax=3)\n",
    "                plt.title(\"Errors: FN(LBlue), TN(Blue), FP(Yellow), TP(Red)\")\n",
    "                plt.axis('off')\n",
    "                plt.colorbar(ticks=[0,1,2,3], label='Error Type')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "evaluator = SegmentationEvaluator(model, X_test, y_test)\n",
    "print(\"Optimal threshold:\", evaluator.find_optimal_threshold())\n",
    "metrics = evaluator.evaluate()\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(f\"Pixel Accuracy: {metrics['pixel_accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"IoU: {metrics['IoU']:.4f}\")\n",
    "print(f\"Dice: {metrics['Dice']:.4f}\")\n",
    "\n",
    "print(\"\\nPixel Counts:\")\n",
    "print(f\"True Positives: {metrics['true_positives']}\")\n",
    "print(f\"False Positives: {metrics['false_positives']}\")\n",
    "print(f\"True Negatives: {metrics['true_negatives']}\")\n",
    "print(f\"False Negatives: {metrics['false_negatives']}\")\n",
    "\n",
    "print(\"\\nObject Detection:\")\n",
    "print(f\"Total Objects: {metrics['total_objects']}\")\n",
    "print(f\"Good Detections: {metrics['good_detections']}\")\n",
    "print(f\"Missed Detections: {metrics['missed_detections']}\")\n",
    "print(f\"False Detections: {metrics['false_detections']}\")\n",
    "print(f\"Detection Rate: {metrics['object_detection_rate']:.2%}\")\n",
    "print(f\"False Discovery Rate: {metrics['false_discovery_rate']:.2%}\")\n",
    "\n",
    "evaluator.visualize_samples(n_samples=100, show_analysis=True) #piemēru daudzums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d1804-3555-404c-a717-1020ede3f4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
